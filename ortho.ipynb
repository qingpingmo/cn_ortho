{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.use_valedges_as_input = True\n",
    "        self.epochs = 150\n",
    "        self.runs = 10\n",
    "        self.dataset = \"Cora\"\n",
    "        self.batch_size = 1152  \n",
    "        self.testbs = 1152 \n",
    "        self.maskinput = True\n",
    "        self.mplayers = 1\n",
    "        self.nnlayers = 3\n",
    "        self.hiddim = 256\n",
    "        self.ln = True\n",
    "        self.lnnn = True\n",
    "        self.res = False\n",
    "        self.jk = True\n",
    "        self.gnndp = 0.05\n",
    "        self.xdp = 0.7\n",
    "        self.tdp = 0.3\n",
    "        self.gnnedp = 0.0\n",
    "        self.predp = 0.05\n",
    "        self.preedp = 0.4\n",
    "        self.gnnlr = 0.0043\n",
    "        self.prelr = 0.0024\n",
    "        self.beta = 1\n",
    "        self.alpha = 1.0\n",
    "        self.use_xlin = True\n",
    "        self.tailact = True\n",
    "        self.twolayerlin = False\n",
    "        self.increasealpha = False\n",
    "        self.splitsize = -1\n",
    "        self.probscale = 4.3\n",
    "        self.proboffset = 2.8\n",
    "        self.pt = 0.75\n",
    "        self.learnpt = False\n",
    "        self.trndeg = -1\n",
    "        self.tstdeg = -1\n",
    "        self.cndeg = -1\n",
    "        self.predictor = 'cn1'\n",
    "        self.depth = 1\n",
    "        self.model = 'puregcn'\n",
    "        self.save_gemb = False\n",
    "        self.load = None\n",
    "        self.loadmod = False\n",
    "        self.savemod = False\n",
    "        self.savex = False\n",
    "        self.loadx = False\n",
    "        self.cnprob = 0\n",
    "        self.emb_dim=256\n",
    "        self.K=8     #Hop you can modify\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_sparse import SparseTensor\n",
    "import torch as th\n",
    "\n",
    "\n",
    "class NormalBasisConv(MessagePassing):\n",
    "    def __init__(self, fixed=False, kwargs={}):\n",
    "        super(NormalBasisConv, self).__init__()\n",
    "        self.fixed = fixed\n",
    "        if self.fixed:\n",
    "            n_hidden = kwargs['n_hidden']\n",
    "            self.register_buffer('three_term_relations', th.zeros(n_hidden, 3), persistent=False)\n",
    "            self.fixed_relation_stored = False\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        \n",
    "        if isinstance(norm, SparseTensor):\n",
    "            norm = norm.to_dense()\n",
    "        return norm.unsqueeze(-1) * x_j\n",
    "\n",
    "    def forward(self, edge_index,adj, last_h, second_last_h, inner_product_last=None, inner_product_second_last=None, is_training=True):\n",
    "        '''if isinstance(adj, SparseTensor):\n",
    "            row, col, norm = adj.coo()\n",
    "            edge_index = th.stack([row, col], dim=0)\n",
    "            if norm is None:\n",
    "                norm = th.ones(row.size(0), device=row.device)\n",
    "            else:\n",
    "                norm = norm.to_dense()\n",
    "        else:\n",
    "            edge_index = (adj > 0).nonzero(as_tuple=False).t()\n",
    "            norm = adj[edge_index[0], edge_index[1]]\n",
    "\n",
    "        if edge_index.dim() != 2 or edge_index.size(0) != 2:\n",
    "            raise ValueError(\"edge_index should have shape (2, num_edges)\")'''\n",
    "\n",
    "\n",
    "        rst=adj  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''orthogonalize'''\n",
    "        if is_training:\n",
    "            rst, inner_product_last, inner_product_second_last = self.orthogonalize(is_training,rst, last_h, second_last_h)\n",
    "        else:\n",
    "            rst, _, _ = self.orthogonalize(is_training,rst, last_h, second_last_h, inner_product_last, inner_product_second_last)\n",
    "\n",
    "        th.cuda.empty_cache()\n",
    "        return rst, inner_product_last, inner_product_second_last\n",
    "\n",
    "    def orthogonalize(self,is_training, rst, last_h, second_last_h, inner_product_last=None, inner_product_second_last=None):\n",
    "            \"\"\"Orthogonalize `rst` wrt. `last_h` and `second_last_h`.\"\"\"\n",
    "            \n",
    "\n",
    "            if isinstance(rst, SparseTensor):\n",
    "                rst = rst.to_dense()\n",
    "            if isinstance(last_h, SparseTensor):\n",
    "                last_h = last_h.to_dense()\n",
    "            if isinstance(second_last_h, SparseTensor):\n",
    "                second_last_h = second_last_h.to_dense()\n",
    "            \n",
    "\n",
    "            if is_training:\n",
    "                \n",
    "\n",
    "                inner_product_last = th.matmul(rst, last_h.T)\n",
    "                \n",
    "\n",
    "                rst = rst - last_h* inner_product_last\n",
    "\n",
    "            else:\n",
    "                inner_product_last = th.zeros(1, device=rst.device)\n",
    "\n",
    "\n",
    "            if is_training:\n",
    "                \n",
    "\n",
    "                inner_product_second_last = th.matmul(rst, second_last_h.T)\n",
    "\n",
    "                \n",
    "                rst = rst - second_last_h * inner_product_second_last\n",
    "            else:\n",
    "                inner_product_second_last = th.zeros(1, device=rst.device)\n",
    "\n",
    "            rst = rst / th.clamp((th.norm(rst,dim=0)),1e-8)\n",
    "            \n",
    "            return rst, inner_product_last, inner_product_second_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_sparse import SparseTensor\n",
    "from torch import Tensor\n",
    "import torch_sparse\n",
    "from typing import List, Tuple, Final\n",
    "\n",
    "\n",
    "class PermIterator:\n",
    "    def __init__(self, device, size, bs, training=True) -> None:\n",
    "        self.bs = bs\n",
    "        self.training = training\n",
    "        self.idx = torch.randperm(size, device=device) if training else torch.arange(size, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.idx.shape[0] + (self.bs - 1) * (not self.training)) // self.bs\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.ptr = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.ptr + self.bs * self.training > self.idx.shape[0]:\n",
    "            raise StopIteration\n",
    "        ret = self.idx[self.ptr:self.ptr + self.bs]\n",
    "        self.ptr += self.bs\n",
    "        return ret\n",
    "\n",
    "def generate_base_adj(data, split_edge, maskinput, batch_size):\n",
    "    pos_train_edge = split_edge['train']['edge'].to(data.x.device).t()\n",
    "    adjmask = torch.ones_like(pos_train_edge[0], dtype=torch.bool)\n",
    "\n",
    "\n",
    "    for perm in PermIterator(adjmask.device, adjmask.shape[0], batch_size):\n",
    "        if maskinput:\n",
    "            adjmask[perm] = 0\n",
    "            tei = pos_train_edge[:, adjmask]\n",
    "            adj = SparseTensor.from_edge_index(\n",
    "                tei, sparse_sizes=(data.num_nodes, data.num_nodes)\n",
    "            ).to(pos_train_edge.device) \n",
    "\n",
    "            adjmask[perm] = 1\n",
    "            adj = adj.to_symmetric()\n",
    "        else:\n",
    "            adj = data.adj_t\n",
    "\n",
    "    return adj\n",
    "\n",
    "def generate_adjs_for_k(data, split_edge, K, args, maskinput, batch_size):\n",
    "    \n",
    "    adj_list = []  \n",
    "\n",
    "    if maskinput:\n",
    "        \n",
    "        pos_train_edge = split_edge['train']['edge'].to(data.x.device).t()\n",
    "        \n",
    "        for k in range(1, K + 1):\n",
    "            adj_k_hop_list = []\n",
    "            for perm in PermIterator(pos_train_edge.device, pos_train_edge.shape[1], batch_size):\n",
    "                adjmask = torch.ones_like(pos_train_edge[0], dtype=torch.bool)\n",
    "                adjmask[perm] = 0\n",
    "                tei = pos_train_edge[:, adjmask]\n",
    "\n",
    "                adj = SparseTensor.from_edge_index(\n",
    "                    tei, sparse_sizes=(data.num_nodes, data.num_nodes)\n",
    "                ).to(pos_train_edge.device)\n",
    "                adj = adj.to_symmetric()\n",
    "                \n",
    "                #k-hop adj\n",
    "                adj_k_hop = generate_adj_k_hop(adj, k)  \n",
    "                adj_k_hop_list.append(adj_k_hop)\n",
    "            \n",
    "            adj_list.append(adj_k_hop_list)\n",
    "\n",
    "    else:\n",
    "\n",
    "        base_adj = data.adj_t  \n",
    "\n",
    "        \n",
    "        for k in range(1, K + 1):\n",
    "            adj_k_hop = generate_adj_k_hop(base_adj, k)\n",
    "            adj_list.append(adj_k_hop)\n",
    "\n",
    "    return adj_list \n",
    "\n",
    "\n",
    "\n",
    "def generate_adj_0_1_hop(adj_1):\n",
    "    \n",
    "    device = adj_1.device()  \n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "        loop_edge = torch.arange(adj_1.size(0), dtype=torch.int64, device=device)\n",
    "        loop_edge = torch.stack([loop_edge, loop_edge], dim=0)  \n",
    "        \n",
    "    except TypeError as e:\n",
    "        \n",
    "        raise\n",
    "\n",
    "    return SparseTensor.from_edge_index(loop_edge).to(device) \n",
    "\n",
    "\n",
    "def generate_adj_k_hop(adj, k):\n",
    "\n",
    "    result_adj = adj\n",
    "    for _ in range(k - 1):  \n",
    "        result_adj = result_adj.matmul(adj)\n",
    "        \n",
    "    return result_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_sparse import SparseTensor\n",
    "from torch import Tensor\n",
    "import torch_sparse\n",
    "from typing import List, Tuple, Final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sparsesample(adj: SparseTensor, deg: int) -> SparseTensor:\n",
    "    '''\n",
    "    sampling elements from a adjacency matrix\n",
    "    '''\n",
    "    rowptr, col, _ = adj.csr()\n",
    "    rowcount = adj.storage.rowcount()\n",
    "    mask = rowcount > 0\n",
    "    rowcount = rowcount[mask]\n",
    "    rowptr = rowptr[:-1][mask]\n",
    "\n",
    "    rand = torch.rand((rowcount.size(0), deg), device=col.device)\n",
    "    rand.mul_(rowcount.to(rand.dtype).reshape(-1, 1))\n",
    "    rand = rand.to(torch.long)\n",
    "    rand.add_(rowptr.reshape(-1, 1))\n",
    "\n",
    "    samplecol = col[rand]\n",
    "\n",
    "    samplerow = torch.arange(adj.size(0), device=adj.device())[mask]\n",
    "\n",
    "    ret = SparseTensor(row=samplerow.reshape(-1, 1).expand(-1, deg).flatten(),\n",
    "                       col=samplecol.flatten(),\n",
    "                       sparse_sizes=adj.sparse_sizes()).to_device(\n",
    "                           adj.device()).coalesce().fill_value_(1.0)\n",
    "    #print(ret.storage.value())\n",
    "    return ret\n",
    "\n",
    "\n",
    "def sparsesample2(adj: SparseTensor, deg: int) -> SparseTensor:\n",
    "    '''\n",
    "    another implementation for sampling elements from a adjacency matrix\n",
    "    '''\n",
    "    rowptr, col, _ = adj.csr()\n",
    "    rowcount = adj.storage.rowcount()\n",
    "    mask = rowcount > deg\n",
    "\n",
    "    rowcount = rowcount[mask]\n",
    "    rowptr = rowptr[:-1][mask]\n",
    "\n",
    "    rand = torch.rand((rowcount.size(0), deg), device=col.device)\n",
    "    rand.mul_(rowcount.to(rand.dtype).reshape(-1, 1))\n",
    "    rand = rand.to(torch.long)\n",
    "    rand.add_(rowptr.reshape(-1, 1))\n",
    "\n",
    "    samplecol = col[rand].flatten()\n",
    "\n",
    "    samplerow = torch.arange(adj.size(0), device=adj.device())[mask].reshape(\n",
    "        -1, 1).expand(-1, deg).flatten()\n",
    "\n",
    "    mask = torch.logical_not(mask)\n",
    "    nosamplerow, nosamplecol = adj[mask].coo()[:2]\n",
    "    nosamplerow = torch.arange(adj.size(0),\n",
    "                               device=adj.device())[mask][nosamplerow]\n",
    "\n",
    "    ret = SparseTensor(\n",
    "        row=torch.cat((samplerow, nosamplerow)),\n",
    "        col=torch.cat((samplecol, nosamplecol)),\n",
    "        sparse_sizes=adj.sparse_sizes()).to_device(\n",
    "            adj.device()).fill_value_(1.0).coalesce()  \n",
    "    #assert (ret.sum(dim=-1) == torch.clip(adj.sum(dim=-1), 0, deg)).all()\n",
    "    return ret\n",
    "\n",
    "\n",
    "def sparsesample_reweight(adj: SparseTensor, deg: int) -> SparseTensor:\n",
    "    '''\n",
    "    another implementation for sampling elements from a adjacency matrix. It will also scale the sampled elements.\n",
    "    \n",
    "    '''\n",
    "    rowptr, col, _ = adj.csr()\n",
    "    rowcount = adj.storage.rowcount()\n",
    "    mask = rowcount > deg\n",
    "\n",
    "    rowcount = rowcount[mask]\n",
    "    rowptr = rowptr[:-1][mask]\n",
    "\n",
    "    rand = torch.rand((rowcount.size(0), deg), device=col.device)\n",
    "    rand.mul_(rowcount.to(rand.dtype).reshape(-1, 1))\n",
    "    rand = rand.to(torch.long)\n",
    "    rand.add_(rowptr.reshape(-1, 1))\n",
    "\n",
    "    samplecol = col[rand].flatten()\n",
    "\n",
    "    samplerow = torch.arange(adj.size(0), device=adj.device())[mask].reshape(\n",
    "        -1, 1).expand(-1, deg).flatten()\n",
    "    samplevalue = (rowcount * (1/deg)).reshape(-1, 1).expand(-1, deg).flatten()\n",
    "\n",
    "    mask = torch.logical_not(mask)\n",
    "    nosamplerow, nosamplecol = adj[mask].coo()[:2]\n",
    "    nosamplerow = torch.arange(adj.size(0),\n",
    "                               device=adj.device())[mask][nosamplerow]\n",
    "\n",
    "    ret = SparseTensor(row=torch.cat((samplerow, nosamplerow)),\n",
    "                       col=torch.cat((samplecol, nosamplecol)),\n",
    "                       value=torch.cat((samplevalue,\n",
    "                                        torch.ones_like(nosamplerow))),\n",
    "                       sparse_sizes=adj.sparse_sizes()).to_device(\n",
    "                           adj.device()).coalesce()  \n",
    "    #assert (ret.sum(dim=-1) == torch.clip(adj.sum(dim=-1), 0, deg)).all()\n",
    "    return ret\n",
    "\n",
    "\n",
    "def elem2spm(element: Tensor, sizes: List[int]) -> SparseTensor:\n",
    "    # Convert adjacency matrix to a 1-d vector\n",
    "    col = torch.bitwise_and(element, 0xffffffff)\n",
    "    row = torch.bitwise_right_shift(element, 32)\n",
    "    return SparseTensor(row=row, col=col, sparse_sizes=sizes).to_device(\n",
    "        element.device).fill_value_(1.0)\n",
    "\n",
    "\n",
    "def spm2elem(spm: SparseTensor) -> Tensor:\n",
    "    # Convert 1-d vector to an adjacency matrix\n",
    "    sizes = spm.sizes()\n",
    "    elem = torch.bitwise_left_shift(spm.storage.row(),\n",
    "                                    32).add_(spm.storage.col())\n",
    "    #elem = spm.storage.row()*sizes[-1] + spm.storage.col()\n",
    "    #assert torch.all(torch.diff(elem) > 0)\n",
    "    return elem\n",
    "\n",
    "\n",
    "def spmoverlap_(adj1: SparseTensor, adj2: SparseTensor) -> SparseTensor:\n",
    "    '''\n",
    "    Compute the overlap of neighbors (rows in adj). The returned matrix is similar to the hadamard product of adj1 and adj2\n",
    "    '''\n",
    "    assert adj1.sizes() == adj2.sizes()\n",
    "    element1 = spm2elem(adj1)\n",
    "    element2 = spm2elem(adj2)\n",
    "\n",
    "    if element2.shape[0] > element1.shape[0]:\n",
    "        element1, element2 = element2, element1\n",
    "\n",
    "    idx = torch.searchsorted(element1[:-1], element2)\n",
    "    mask = (element1[idx] == element2)\n",
    "    retelem = element2[mask]\n",
    "    '''\n",
    "    nnz1 = adj1.nnz()\n",
    "    element = torch.cat((adj1.storage.row(), adj2.storage.row()), dim=-1)\n",
    "    element.bitwise_left_shift_(32)\n",
    "    element[:nnz1] += adj1.storage.col()\n",
    "    element[nnz1:] += adj2.storage.col()\n",
    "    \n",
    "    element = torch.sort(element, dim=-1)[0]\n",
    "    mask = (element[1:] == element[:-1])\n",
    "    retelem = element[:-1][mask]\n",
    "    '''\n",
    "\n",
    "    return elem2spm(retelem, adj1.sizes())\n",
    "\n",
    "\n",
    "def spmnotoverlap_(adj1: SparseTensor,\n",
    "                   adj2: SparseTensor) -> Tuple[SparseTensor, SparseTensor]:\n",
    "    '''\n",
    "    return elements in adj1 but not in adj2 and in adj2 but not adj1\n",
    "    '''\n",
    "    # assert adj1.sizes() == adj2.sizes()\n",
    "    element1 = spm2elem(adj1)\n",
    "    element2 = spm2elem(adj2)\n",
    "\n",
    "    idx = torch.searchsorted(element1[:-1], element2)\n",
    "    matchedmask = (element1[idx] == element2)\n",
    "\n",
    "    maskelem1 = torch.ones_like(element1, dtype=torch.bool)\n",
    "    maskelem1[idx[matchedmask]] = 0\n",
    "    retelem1 = element1[maskelem1]\n",
    "\n",
    "    retelem2 = element2[torch.logical_not(matchedmask)]\n",
    "    return elem2spm(retelem1, adj1.sizes()), elem2spm(retelem2, adj2.sizes())\n",
    "\n",
    "\n",
    "def spmoverlap_notoverlap_(\n",
    "        adj1: SparseTensor,\n",
    "        adj2: SparseTensor) -> Tuple[SparseTensor, SparseTensor, SparseTensor]:\n",
    "    '''\n",
    "    return elements in adj1 but not in adj2 and in adj2 but not adj1\n",
    "    '''\n",
    "    # assert adj1.sizes() == adj2.sizes()\n",
    "    element1 = spm2elem(adj1)\n",
    "    element2 = spm2elem(adj2)\n",
    "\n",
    "    if element1.shape[0] == 0:\n",
    "        retoverlap = element1\n",
    "        retelem1 = element1\n",
    "        retelem2 = element2\n",
    "    else:\n",
    "        idx = torch.searchsorted(element1[:-1], element2)\n",
    "        matchedmask = (element1[idx] == element2)\n",
    "\n",
    "        maskelem1 = torch.ones_like(element1, dtype=torch.bool)\n",
    "        maskelem1[idx[matchedmask]] = 0\n",
    "        retelem1 = element1[maskelem1]\n",
    "\n",
    "        retoverlap = element2[matchedmask]\n",
    "        retelem2 = element2[torch.logical_not(matchedmask)]\n",
    "    sizes = adj1.sizes()\n",
    "    return elem2spm(retoverlap,\n",
    "                    sizes), elem2spm(retelem1,\n",
    "                                     sizes), elem2spm(retelem2, sizes)\n",
    "\n",
    "\n",
    "def adjoverlap(adj1: SparseTensor, \n",
    "               adj2: SparseTensor, \n",
    "               tarei: Tensor, \n",
    "               filled1: bool = False, \n",
    "               calresadj: bool = False, \n",
    "               cnsampledeg: int = -1, \n",
    "               ressampledeg: int = -1) -> SparseTensor:\n",
    "    \"\"\"\n",
    "    A wrapper for functions above to compute adjacency overlaps.\n",
    "    Ensure inputs are SparseTensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(adj1, SparseTensor):\n",
    "        adj1 = SparseTensor.from_dense(adj1)\n",
    "    if not isinstance(adj2, SparseTensor):\n",
    "        adj2 = SparseTensor.from_dense(adj2)\n",
    "\n",
    "   \n",
    "    adj1 = adj1[tarei[0]]\n",
    "    adj2 = adj2[tarei[1]]\n",
    "\n",
    "    if calresadj:\n",
    "        \n",
    "        adjoverlap, adjres1, adjres2 = spmoverlap_notoverlap_(adj1, adj2)\n",
    "        if cnsampledeg > 0:\n",
    "            adjoverlap = sparsesample_reweight(adjoverlap, cnsampledeg)\n",
    "        if ressampledeg > 0:\n",
    "            adjres1 = sparsesample_reweight(adjres1, ressampledeg)\n",
    "            adjres2 = sparsesample_reweight(adjres2, ressampledeg)\n",
    "        return adjoverlap, adjres1, adjres2\n",
    "    else:\n",
    "      \n",
    "        adjoverlap = spmoverlap_(adj1, adj2)\n",
    "        if cnsampledeg > 0:\n",
    "            adjoverlap = sparsesample_reweight(adjoverlap, cnsampledeg)\n",
    "\n",
    "    return adjoverlap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DropAdj(nn.Module):\n",
    "    doscale: Final[bool] \n",
    "\n",
    "    def __init__(self, dp: float = 0.0, doscale=True) -> None:\n",
    "        super().__init__()\n",
    "        self.dp = dp\n",
    "        self.register_buffer(\"ratio\", torch.tensor(1 / (1 - dp)))\n",
    "        self.doscale = doscale\n",
    "\n",
    "    def forward(self, adj: SparseTensor) -> SparseTensor:\n",
    "        if self.dp < 1e-6 or not self.training:\n",
    "            return adj\n",
    "\n",
    "       \n",
    "        adj_dense = adj.to_dense()\n",
    "\n",
    "      \n",
    "        mask = torch.rand_like(adj_dense) > self.dp\n",
    "        adj_dense = adj_dense * mask.float()  # mask\n",
    "\n",
    "        \n",
    "        if self.doscale:\n",
    "            adj_dense = adj_dense * self.ratio\n",
    "\n",
    "       \n",
    "        adj_sparse = SparseTensor.from_dense(adj_dense)\n",
    "\n",
    "        return adj_sparse\n",
    "\n",
    "def sparse_diff(spm_x, spm_y):\n",
    "    \"\"\"\n",
    "    Given 2 sparse tensor spm_x and spm_y, do the diff x - y.\n",
    "    x: bs * nidx, y: bs * nidy\n",
    "    require nidx >= nidy\n",
    "    \"\"\"\n",
    "    return spmoverlap_notoverlap_(spm_x, spm_y)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "#from utils import adjoverlap\n",
    "from torch_sparse.matmul import spmm_max, spmm_mean, spmm_add\n",
    "from torch_sparse import SparseTensor\n",
    "import torch_sparse\n",
    "from torch_scatter import scatter_add\n",
    "from typing import Iterable, Final\n",
    "\n",
    "\n",
    "\n",
    "class PureConv(nn.Module):\n",
    "    aggr: Final[str]\n",
    "    def __init__(self, indim, outdim, aggr=\"gcn\") -> None:\n",
    "        super().__init__()\n",
    "        self.aggr = aggr\n",
    "        if indim == outdim:\n",
    "            self.lin = nn.Identity()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.lin(x)\n",
    "        if self.aggr == \"mean\":\n",
    "            return spmm_mean(adj_t, x)\n",
    "        elif self.aggr == \"max\":\n",
    "            return spmm_max(adj_t, x)[0]\n",
    "        elif self.aggr == \"sum\":\n",
    "            return spmm_add(adj_t, x)\n",
    "        elif self.aggr == \"gcn\":\n",
    "            norm = torch.rsqrt_((1+adj_t.sum(dim=-1))).reshape(-1, 1)\n",
    "            x = norm * x\n",
    "            x = spmm_add(adj_t, x) + x\n",
    "            x = norm * x\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "convdict = {\n",
    "    \"gcn\":\n",
    "    GCNConv,\n",
    "    \"gcn_cached\":\n",
    "    lambda indim, outdim: GCNConv(indim, outdim, cached=True),\n",
    "    \"sage\":\n",
    "    lambda indim, outdim: GCNConv(\n",
    "        indim, outdim, aggr=\"mean\", normalize=False, add_self_loops=False),\n",
    "    \"gin\":\n",
    "    lambda indim, outdim: GCNConv(\n",
    "        indim, outdim, aggr=\"sum\", normalize=False, add_self_loops=False),\n",
    "    \"max\":\n",
    "    lambda indim, outdim: GCNConv(\n",
    "        indim, outdim, aggr=\"max\", normalize=False, add_self_loops=False),\n",
    "    \"puremax\": \n",
    "    lambda indim, outdim: PureConv(indim, outdim, aggr=\"max\"),\n",
    "    \"puresum\": \n",
    "    lambda indim, outdim: PureConv(indim, outdim, aggr=\"sum\"),\n",
    "    \"puremean\": \n",
    "    lambda indim, outdim: PureConv(indim, outdim, aggr=\"mean\"),\n",
    "    \"puregcn\": \n",
    "    lambda indim, outdim: PureConv(indim, outdim, aggr=\"gcn\"),\n",
    "    \"none\":\n",
    "    None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch_sparse.matmul import spmm_max, spmm_mean, spmm_add\n",
    "from torch_sparse import SparseTensor\n",
    "import torch_sparse\n",
    "from torch_scatter import scatter_add\n",
    "from typing import Iterable, Final\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 hidden_channels,\n",
    "                 out_channels,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 ln=False,\n",
    "                 res=False,\n",
    "                 max_x=-1,\n",
    "                 conv_fn=\"gcn\",\n",
    "                 jk=False,\n",
    "                 edrop=0.0,\n",
    "                 xdropout=0.0,\n",
    "                 taildropout=0.0,\n",
    "                 noinputlin=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.adjdrop = DropAdj(edrop)\n",
    "        \n",
    "        if max_x >= 0:\n",
    "            tmp = nn.Embedding(max_x + 1, hidden_channels)\n",
    "            nn.init.orthogonal_(tmp.weight)\n",
    "            self.xemb = nn.Sequential(tmp, nn.Dropout(dropout))\n",
    "            in_channels = hidden_channels\n",
    "        else:\n",
    "            self.xemb = nn.Sequential(nn.Dropout(xdropout)) #nn.Identity()\n",
    "            if not noinputlin and (\"pure\" in conv_fn or num_layers==0):\n",
    "                self.xemb.append(nn.Linear(in_channels, hidden_channels))\n",
    "                self.xemb.append(nn.Dropout(dropout, inplace=True) if dropout > 1e-6 else nn.Identity())\n",
    "        \n",
    "        self.res = res\n",
    "        self.jk = jk\n",
    "        if jk:\n",
    "            self.register_parameter(\"jkparams\", nn.Parameter(torch.randn((num_layers,))))\n",
    "            \n",
    "        if num_layers == 0 or conv_fn ==\"none\":\n",
    "            self.jk = False\n",
    "            return\n",
    "        \n",
    "        convfn = convdict[conv_fn]\n",
    "        lnfn = lambda dim, ln: nn.LayerNorm(dim) if ln else nn.Identity()\n",
    "\n",
    "        if num_layers == 1:\n",
    "            hidden_channels = out_channels\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.lins = nn.ModuleList()\n",
    "        if \"pure\" in conv_fn:\n",
    "            self.convs.append(convfn(hidden_channels, hidden_channels))\n",
    "            for i in range(num_layers-1):\n",
    "                self.lins.append(nn.Identity())\n",
    "                self.convs.append(convfn(hidden_channels, hidden_channels))\n",
    "            self.lins.append(nn.Dropout(taildropout, True))\n",
    "        else:\n",
    "            self.convs.append(convfn(in_channels, hidden_channels))\n",
    "            self.lins.append(\n",
    "                nn.Sequential(lnfn(hidden_channels, ln), nn.Dropout(dropout, True),\n",
    "                            nn.ReLU(True)))\n",
    "            for i in range(num_layers - 1):\n",
    "                self.convs.append(\n",
    "                    convfn(\n",
    "                        hidden_channels,\n",
    "                        hidden_channels if i == num_layers - 2 else out_channels))\n",
    "                if i < num_layers - 2:\n",
    "                    self.lins.append(\n",
    "                        nn.Sequential(\n",
    "                            lnfn(\n",
    "                                hidden_channels if i == num_layers -\n",
    "                                2 else out_channels, ln),\n",
    "                            nn.Dropout(dropout, True), nn.ReLU(True)))\n",
    "                else:\n",
    "                    self.lins.append(nn.Identity())\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.xemb(x)\n",
    "        jkx = []\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x1 = self.lins[i](conv(x, self.adjdrop(adj_t)))\n",
    "            if self.res and x1.shape[-1] == x.shape[-1]: # residual connection\n",
    "                x = x1 + x\n",
    "            else:\n",
    "                x = x1\n",
    "            if self.jk:\n",
    "                jkx.append(x)\n",
    "        if self.jk: # JumpingKnowledge Connection\n",
    "            jkx = torch.stack(jkx, dim=0)\n",
    "            sftmax = self.jkparams.reshape(-1, 1, 1)\n",
    "            x = torch.sum(jkx*sftmax, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_sparse import SparseTensor\n",
    "from typing import Iterable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CNLinkPredictor(nn.Module):\n",
    "    cndeg: int\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 hidden_channels,\n",
    "                 out_channels,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 edrop=0.0,\n",
    "                 ln=False,\n",
    "                 cndeg=-1,\n",
    "                 use_xlin=False,\n",
    "                 tailact=False,\n",
    "                 twolayerlin=False,\n",
    "                 beta=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_parameter(\"beta\", nn.Parameter(beta * th.ones((1))))\n",
    "        self.dropadj = DropAdj(edrop)\n",
    "        lnfn = lambda dim, ln: nn.LayerNorm(dim) if ln else nn.Identity()\n",
    "\n",
    "        self.xlin = nn.Sequential(nn.Linear(hidden_channels, hidden_channels),\n",
    "                                  nn.Dropout(dropout, inplace=True), nn.ReLU(inplace=True),\n",
    "                                  nn.Linear(hidden_channels, hidden_channels),\n",
    "                                  lnfn(hidden_channels, ln), nn.Dropout(dropout, inplace=True), nn.ReLU(inplace=True)) if use_xlin else lambda x: 0\n",
    "\n",
    "        self.xcnlin = nn.Sequential(nn.Linear(1, hidden_channels),\n",
    "                                    nn.Dropout(dropout, inplace=True), nn.ReLU(inplace=True),\n",
    "                                    nn.Linear(hidden_channels, hidden_channels),\n",
    "                                    lnfn(hidden_channels, ln), nn.Dropout(dropout, inplace=True),\n",
    "                                    nn.ReLU(inplace=True), nn.Linear(hidden_channels, hidden_channels) if not tailact else nn.Identity())\n",
    "        \n",
    "        self.xijlin = nn.Sequential(nn.Linear(in_channels, hidden_channels), lnfn(hidden_channels, ln),\n",
    "                                    nn.Dropout(dropout, inplace=True), nn.ReLU(inplace=True),\n",
    "                                    nn.Linear(hidden_channels, hidden_channels) if not tailact else nn.Identity())\n",
    "        \n",
    "        self.lin = nn.Sequential(nn.Linear(hidden_channels, hidden_channels),\n",
    "                                 lnfn(hidden_channels, ln),\n",
    "                                 nn.Dropout(dropout, inplace=True),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Linear(hidden_channels, hidden_channels) if twolayerlin else nn.Identity(),\n",
    "                                 lnfn(hidden_channels, ln) if twolayerlin else nn.Identity(),\n",
    "                                 nn.Dropout(dropout, inplace=True) if twolayerlin else nn.Identity(),\n",
    "                                 nn.ReLU(inplace=True) if twolayerlin else nn.Identity(),\n",
    "                                 nn.Linear(hidden_channels, out_channels))\n",
    "        \n",
    "\n",
    "        self.mlp_sparse = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels), \n",
    "            nn.Dropout(dropout, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_channels, hidden_channels),  \n",
    "            lnfn(hidden_channels, ln),  \n",
    "            nn.Dropout(dropout, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_channels, hidden_channels) if not tailact else nn.Identity()\n",
    "        )\n",
    "\n",
    "  \n",
    "        self.mlp_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.Dropout(dropout, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_channels, hidden_channels), \n",
    "            lnfn(hidden_channels, ln),\n",
    "            nn.Dropout(dropout, inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_channels, hidden_channels) if twolayerlin else nn.Identity(),\n",
    "            lnfn(hidden_channels, ln) if twolayerlin else nn.Identity(),\n",
    "            nn.Dropout(dropout, inplace=True) if twolayerlin else nn.Identity(),\n",
    "            nn.ReLU(inplace=True) if twolayerlin else nn.Identity(),\n",
    "            nn.Linear(hidden_channels, out_channels)  \n",
    "        )\n",
    "\n",
    "\n",
    "        self.cndeg = cndeg\n",
    "        self.K = args.K\n",
    "        self.convs = nn.ModuleList([NormalBasisConv() for _ in range(self.K)])\n",
    "        self.ln=ln\n",
    "\n",
    "        self.accumulated_inner_products = []\n",
    "        self.batch_count = 0\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.current_batch_progress = 0 \n",
    "        self.batch_size = None\n",
    "        self.dynamic_xcnlin = None\n",
    "        self.dropout=dropout\n",
    "        self.tailact=tailact\n",
    "\n",
    "    def initialize_xcnlin(self, adj_size):\n",
    "        \"\"\"Initialize self.xcnlin based on adjs[0].size(1).\"\"\"\n",
    "        lnfn = lambda dim, ln: nn.LayerNorm(dim) if self.ln else nn.Identity()\n",
    "        if self.dynamic_xcnlin is None:  \n",
    "            in_channels = adj_size[1]\n",
    "            lnfn = lambda dim: nn.LayerNorm(dim) if self.ln else nn.Identity()\n",
    "            self.dynamic_xcnlin = nn.Sequential(\n",
    "                nn.Linear(in_channels, self.hidden_channels),\n",
    "                nn.Dropout(self.dropout, inplace=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(self.hidden_channels, self.hidden_channels),\n",
    "                lnfn(self.hidden_channels),\n",
    "                nn.Dropout(self.dropout, inplace=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(self.hidden_channels, self.hidden_channels) if not self.tailact else nn.Identity()\n",
    "            ).to(device)\n",
    "\n",
    "    def reset_constants(self, device, size_x):\n",
    "        self.accumulated_inner_products = []\n",
    "\n",
    "\n",
    "    def accumulate_constants(self, inner_product_sums, batch_size, device, batch_id):\n",
    "        \n",
    "        while len(self.accumulated_inner_products) <= batch_id:\n",
    "            self.accumulated_inner_products.append([th.zeros_like(inner_product_sums[0][j], device=device) for j in range(2 * self.K)])\n",
    "\n",
    "        for j in range(2 * self.K):\n",
    "\n",
    "            if batch_id > 0:\n",
    "                \n",
    "                self.accumulated_inner_products[batch_id][j] = self.accumulated_inner_products[batch_id-1][j].to(device) + \\\n",
    "                    (1 / batch_id) * (inner_product_sums[batch_id][j] - self.accumulated_inner_products[batch_id-1][j])\n",
    "                \n",
    "            else:\n",
    "               \n",
    "                self.accumulated_inner_products[batch_id][j] = inner_product_sums[batch_id][j]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def multidomainforward(self, x, adjs, tar_ei, filled1: bool = False, cndropprobs: Iterable[float] = [], batch_idx=None, is_training=True):\n",
    "       \n",
    "        device = x.device\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = tar_ei.size(1)\n",
    "\n",
    "        if self.dynamic_xcnlin is None:\n",
    "            self.initialize_xcnlin(adjs[0].size())\n",
    "\n",
    "        if any(isinstance(adj, list) for adj in adjs):\n",
    "            adjs = list(itertools.chain.from_iterable(adjs))\n",
    "\n",
    "        xi = x[tar_ei[0]]\n",
    "        xj = x[tar_ei[1]]\n",
    "        xij = self.xijlin(xi * xj)\n",
    "\n",
    "        x = x + self.xlin(x)\n",
    "\n",
    "        \n",
    "        cn_k=[]\n",
    "        for k in range(args.K):\n",
    "            cn_k.append(adjs[k])\n",
    "\n",
    "        sparse_diffs=[]\n",
    "        for k in range(args.K):\n",
    "            sparse_diffs.append(cn_k[k])\n",
    "\n",
    "       \n",
    "        h0 = sparse_diffs[0] / th.clamp((th.norm(sparse_diffs[0],dim=0)), 1e-8)\n",
    "    \n",
    "        inner_product_sums = []\n",
    "        if batch_idx is not None:\n",
    "            while len(inner_product_sums) <= batch_idx:\n",
    "                inner_product_sums.append([th.zeros((1, 1), device=device) for _ in range(2 * self.K)])\n",
    "       \n",
    "        second_last_h = th.zeros_like(h0, device=device)\n",
    "        last_h = h0\n",
    "\n",
    "        for i, (con, adj) in enumerate(zip(self.convs, sparse_diffs[1:]), 1):\n",
    "            adj = adj.to(device)\n",
    "            h_i = None  \n",
    "\n",
    "            if is_training:\n",
    "                h_i, inner_product_last, inner_product_second_last = con(tar_ei,adj, last_h, second_last_h, is_training=True)\n",
    "\n",
    "\n",
    "                \n",
    "                if batch_idx is not None:\n",
    "                    inner_product_sums[batch_idx][2 * (i - 1)] += inner_product_last.to(device)\n",
    "                    \n",
    "\n",
    "                    inner_product_sums[batch_idx][2 * (i - 1) + 1] += inner_product_second_last.to(device)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                inner_product_last = self.accumulated_inner_products[2][2 * (i - 1)].to(device)\n",
    "\n",
    "\n",
    "                inner_product_second_last = self.accumulated_inner_products[2][2 * (i - 1) + 1].to(device)\n",
    "\n",
    "\n",
    "                h_i, _, _ = con(adj, last_h, second_last_h, inner_product_last, inner_product_second_last, is_training=False)\n",
    "\n",
    "            if h_i is not None:\n",
    "                second_last_h = last_h\n",
    "                last_h = h_i.to(device)\n",
    "\n",
    "                sparse_diffs[i] = h_i.to(device)\n",
    "\n",
    "\n",
    "        if is_training:\n",
    "            self.accumulate_constants(inner_product_sums, tar_ei.size(1), device, batch_idx)\n",
    "          \n",
    "\n",
    "      \n",
    "\n",
    "        xs = []\n",
    "\n",
    "        for xcn in sparse_diffs[k]:\n",
    "            \n",
    "            xcn_expanded = xcn.unsqueeze(-1).repeat(1, self.hidden_channels)  # (1, h) -> (h, hidden_channel)\n",
    "\n",
    "            xcn_transformed = self.mlp_sparse(xcn_expanded.to(device))\n",
    "\n",
    "            fused = xcn_transformed * self.beta + xij  \n",
    "\n",
    "            fusion_transformed = self.mlp_fusion(fused.to(device))\n",
    " \n",
    "            xs.append(fusion_transformed)\n",
    "\n",
    "\n",
    "        xs = torch.cat(xs, dim=-1)\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def forward(self, x, adj, tar_ei, filled1: bool = False, is_training=True):\n",
    "        return self.multidomainforward(x, adj, tar_ei, filled1, [], None, is_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_dict = {\n",
    "\n",
    "    \"cn1\": CNLinkPredictor\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling, to_undirected\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def randomsplit(dataset, val_ratio: float=0.10, test_ratio: float=0.2):\n",
    "    def removerepeated(ei):\n",
    "        ei = to_undirected(ei)\n",
    "        ei = ei[:, ei[0]<ei[1]]\n",
    "        return ei\n",
    "    data = dataset[0]\n",
    "    data.num_nodes = data.x.shape[0]\n",
    "    data = train_test_split_edges(data, test_ratio, test_ratio)\n",
    "    split_edge = {'train': {}, 'valid': {}, 'test': {}}\n",
    "    num_val = int(data.val_pos_edge_index.shape[1] * val_ratio/test_ratio)\n",
    "    data.val_pos_edge_index = data.val_pos_edge_index[:, torch.randperm(data.val_pos_edge_index.shape[1])]\n",
    "    split_edge['train']['edge'] = removerepeated(torch.cat((data.train_pos_edge_index, data.val_pos_edge_index[:, :-num_val]), dim=-1)).t()\n",
    "    split_edge['valid']['edge'] = removerepeated(data.val_pos_edge_index[:, -num_val:]).t()\n",
    "    split_edge['valid']['edge_neg'] = removerepeated(data.val_neg_edge_index).t()\n",
    "    split_edge['test']['edge'] = removerepeated(data.test_pos_edge_index).t()\n",
    "    split_edge['test']['edge_neg'] = removerepeated(data.test_neg_edge_index).t()\n",
    "    return split_edge\n",
    "\n",
    "def loaddataset(name: str, use_valedges_as_input: bool, load=None):\n",
    "    if name in [\"Cora\", \"Citeseer\", \"Pubmed\"]:\n",
    "        dataset = Planetoid(root=\"dataset\", name=name)\n",
    "        split_edge = randomsplit(dataset)\n",
    "        data = dataset[0]\n",
    "        data.edge_index = to_undirected(split_edge[\"train\"][\"edge\"].t())\n",
    "        edge_index = data.edge_index\n",
    "        data.num_nodes = data.x.shape[0]\n",
    "    else:\n",
    "        dataset = PygLinkPropPredDataset(name=f'ogbl-{name}')\n",
    "        split_edge = dataset.get_edge_split()\n",
    "        data = dataset[0]\n",
    "        edge_index = data.edge_index\n",
    "    data.edge_weight = None \n",
    "    print(data.num_nodes, edge_index.max())\n",
    "    data.adj_t = SparseTensor.from_edge_index(edge_index, sparse_sizes=(data.num_nodes, data.num_nodes))\n",
    "    data.adj_t = data.adj_t.to_symmetric().coalesce()\n",
    "    data.max_x = -1\n",
    "    if name == \"ppa\":\n",
    "        data.x = torch.argmax(data.x, dim=-1)\n",
    "        data.max_x = torch.max(data.x).item()\n",
    "    elif name == \"ddi\":\n",
    "        data.x = torch.arange(data.num_nodes)\n",
    "        data.max_x = data.num_nodes\n",
    "    if load is not None:\n",
    "        data.x = torch.load(load, map_location=\"cpu\")\n",
    "        data.max_x = -1\n",
    "\n",
    "    print(\"dataset split \")\n",
    "    for key1 in split_edge:\n",
    "        for key2  in split_edge[key1]:\n",
    "            print(key1, key2, split_edge[key1][key2].shape[0])\n",
    "\n",
    "\n",
    "    if use_valedges_as_input:\n",
    "        val_edge_index = split_edge['valid']['edge'].t()\n",
    "        full_edge_index = torch.cat([edge_index, val_edge_index], dim=-1)\n",
    "        data.full_adj_t = SparseTensor.from_edge_index(full_edge_index, sparse_sizes=(data.num_nodes, data.num_nodes)).coalesce()\n",
    "        data.full_adj_t = data.full_adj_t.to_symmetric()\n",
    "    else:\n",
    "        data.full_adj_t = data.adj_t\n",
    "    return data, split_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x77db9e674e80>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2927963 tensor(2927962)\n",
      "dataset split \n",
      "train source_node 30387995\n",
      "train target_node 30387995\n",
      "valid source_node 86596\n",
      "valid target_node 86596\n",
      "valid target_node_neg 86596\n",
      "test source_node 86596\n",
      "test target_node 86596\n",
      "test target_node_neg 86596\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'edge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 334\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal result: val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maverage(ret[:, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(ret[:, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tst \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maverage(ret[:, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(ret[:, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 334\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 235\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m Evaluator(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mogbl-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    234\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m data, split_edge \u001b[38;5;241m=\u001b[39m \u001b[43mloaddataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_valedges_as_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    238\u001b[0m predfn \u001b[38;5;241m=\u001b[39m predictor_dict[args\u001b[38;5;241m.\u001b[39mpredictor]\n",
      "Cell \u001b[0;32mIn[54], line 66\u001b[0m, in \u001b[0;36mloaddataset\u001b[0;34m(name, use_valedges_as_input, load)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(key1, key2, split_edge[key1][key2]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_valedges_as_input:\n\u001b[0;32m---> 66\u001b[0m     val_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43msplit_edge\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43medge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m     67\u001b[0m     full_edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index, val_edge_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m     data\u001b[38;5;241m.\u001b[39mfull_adj_t \u001b[38;5;241m=\u001b[39m SparseTensor\u001b[38;5;241m.\u001b[39mfrom_edge_index(full_edge_index, sparse_sizes\u001b[38;5;241m=\u001b[39m(data\u001b[38;5;241m.\u001b[39mnum_nodes, data\u001b[38;5;241m.\u001b[39mnum_nodes))\u001b[38;5;241m.\u001b[39mcoalesce()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'edge'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_sparse import SparseTensor\n",
    "import torch_geometric.transforms as T\n",
    "from functools import partial\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "from typing import Iterable\n",
    "\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "from torch_sparse import SparseTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def CN_kij(adj: SparseTensor, k: int, ij: torch.Tensor, slice_size: int=512):\n",
    "    device = adj.device() \n",
    "    ij = ij.to(device)  \n",
    "\n",
    "    h = ij.shape[1]  \n",
    "   \n",
    "    n = adj.size(0)\n",
    "    \n",
    "    rets = []\n",
    "    \n",
    "   \n",
    "    for idx in range(0, h, slice_size):\n",
    "        tij = ij[:, idx: idx + slice_size]\n",
    "        Ej = F.one_hot(tij[1].long(), num_classes=n).T.float().to(device) \n",
    "        \n",
    "        Ei = F.one_hot(tij[0].long(), num_classes=n).float().to(device)   \n",
    "       \n",
    "        ret = []\n",
    "       \n",
    "        for _ in range(k): \n",
    "            Ej = adj @ Ej\n",
    "            \n",
    "            ret.append((Ei * Ej.T).sum(dim=-1))\n",
    "           \n",
    "        \n",
    "        ret = torch.stack(ret, dim=-1)  # (slice_size, k)\n",
    "      \n",
    "        rets.append(ret)\n",
    "        \n",
    "\n",
    "  \n",
    "    rets = torch.cat(rets, dim=0)\n",
    "   \n",
    "\n",
    "   \n",
    "    adjs = [rets[:, i].unsqueeze(0) for i in range(k)]\n",
    "   \n",
    "    \n",
    "    return adjs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, predictor, data, split_edge, optimizer, batch_size, maskinput=True, cnprobs=[], alpha=None):\n",
    "    if alpha is not None:\n",
    "        predictor.setalpha(alpha)\n",
    "\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    pos_train_edge = split_edge['train']['edge'].to(data.x.device).t()\n",
    "   \n",
    "\n",
    "    total_loss = []\n",
    "    negedge = negative_sampling(data.edge_index.to(pos_train_edge.device), data.adj_t.sizes()[0])\n",
    " \n",
    "    \n",
    "\n",
    "    for batch_idx, perm in enumerate(PermIterator(pos_train_edge.device, pos_train_edge.shape[1], batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        adjmask = torch.ones_like(pos_train_edge[0], dtype=torch.bool)\n",
    "        if maskinput:\n",
    "            adjmask[perm] = 0\n",
    "            tei = pos_train_edge[:, adjmask]\n",
    "            adj = SparseTensor.from_edge_index(tei, sparse_sizes=(data.num_nodes, data.num_nodes)).to(pos_train_edge.device)\n",
    "            adjmask[perm] = 1\n",
    "            adj = adj.to_symmetric()\n",
    "           \n",
    "        else:\n",
    "            adj = data.adj_t\n",
    "\n",
    "    \n",
    "        h = model(data.x, adj)\n",
    "        edge = pos_train_edge[:, perm]\n",
    "\n",
    "        adjs = CN_kij(adj,args.K,edge)\n",
    "\n",
    "        \n",
    "        pos_outs = predictor.multidomainforward(h, adjs, edge, cndropprobs=cnprobs, batch_idx=batch_idx + 1)\n",
    "        pos_loss = -F.logsigmoid(pos_outs).mean()\n",
    "        \n",
    "        edge = negedge[:, perm]\n",
    "        neg_outs = predictor.multidomainforward(h, adjs, edge, cndropprobs=cnprobs, batch_idx=batch_idx + 1)\n",
    "        neg_loss = -F.logsigmoid(-neg_outs).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        print(f\"[train] Loss: {loss.item()}\")\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "       \n",
    "        del adjmask, tei, adj, pos_outs, neg_outs, h, adjs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.mean(total_loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, predictor, data, split_edge, evaluator, batch_size,\n",
    "         use_valedges_as_input):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    pos_train_edge = split_edge['train']['edge'].to(data.adj_t.device())\n",
    "    pos_valid_edge = split_edge['valid']['edge'].to(data.adj_t.device())\n",
    "    neg_valid_edge = split_edge['valid']['edge_neg'].to(data.adj_t.device())\n",
    "    pos_test_edge = split_edge['test']['edge'].to(data.adj_t.device())\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].to(data.adj_t.device())\n",
    "\n",
    "    adj = data.adj_t\n",
    "    h = model(data.x, adj)\n",
    "\n",
    "    \n",
    "    pos_train_pred = torch.cat([\n",
    "        predictor(h, CN_kij(adj,args.K,pos_train_edge[perm].t()), pos_train_edge[perm].t(), is_training=False).squeeze().cpu()\n",
    "        for perm in PermIterator(pos_train_edge.device,\n",
    "                                 pos_train_edge.shape[0], batch_size, False)\n",
    "    ],\n",
    "                               dim=0)\n",
    "\n",
    "\n",
    "    pos_valid_pred = torch.cat([\n",
    "        predictor(h, CN_kij(adj,args.K,pos_valid_edge[perm].t()), pos_valid_edge[perm].t(), is_training=False).squeeze().cpu()\n",
    "        for perm in PermIterator(pos_valid_edge.device,\n",
    "                                 pos_valid_edge.shape[0], batch_size, False)\n",
    "    ],\n",
    "                               dim=0)\n",
    "    neg_valid_pred = torch.cat([\n",
    "        predictor(h, CN_kij(adj,args.K,neg_valid_edge[perm].t()), neg_valid_edge[perm].t(), is_training=False).squeeze().cpu()\n",
    "        for perm in PermIterator(neg_valid_edge.device,\n",
    "                                 neg_valid_edge.shape[0], batch_size, False)\n",
    "    ],\n",
    "                               dim=0)\n",
    "    if use_valedges_as_input:\n",
    "        adj = data.full_adj_t\n",
    "        h = model(data.x, adj)\n",
    "\n",
    "    pos_test_pred = torch.cat([\n",
    "        predictor(h, CN_kij(adj,args.K,pos_test_edge[perm].t()), pos_test_edge[perm].t(), is_training=False).squeeze().cpu()\n",
    "        for perm in PermIterator(pos_test_edge.device, pos_test_edge.shape[0],\n",
    "                                 batch_size, False)\n",
    "    ],\n",
    "                              dim=0)\n",
    "\n",
    "    neg_test_pred = torch.cat([\n",
    "        predictor(h,  CN_kij(adj,args.K,neg_test_edge[perm].t()), neg_test_edge[perm].t(),is_training=False).squeeze().cpu()\n",
    "        for perm in PermIterator(neg_test_edge.device, neg_test_edge.shape[0],\n",
    "                                 batch_size, False)\n",
    "    ],\n",
    "                              dim=0)\n",
    "\n",
    "    results = {}\n",
    "    for K in [20, 50, 100]:\n",
    "        evaluator.K = K\n",
    "\n",
    "        train_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_train_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "\n",
    "        valid_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_valid_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "        test_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_test_pred,\n",
    "            'y_pred_neg': neg_test_pred,\n",
    "        })[f'hits@{K}']\n",
    "\n",
    "        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n",
    "    return results, h.cpu()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = Args()\n",
    "    print(args, flush=True)\n",
    "\n",
    "    hpstr = str(args).replace(\" \", \"\").replace(\"Namespace(\", \"\").replace(\n",
    "        \")\", \"\").replace(\"True\", \"1\").replace(\"False\", \"0\").replace(\"=\", \"\").replace(\"epochs\", \"\").replace(\"runs\", \"\").replace(\"save_gemb\", \"\")\n",
    "    writer = SummaryWriter(f\"./rec/{args.model}_{args.predictor}\")\n",
    "    writer.add_text(\"hyperparams\", hpstr)\n",
    "\n",
    "    if args.dataset in [\"Cora\", \"Citeseer\", \"Pubmed\"]:\n",
    "        evaluator = Evaluator(name=f'ogbl-ppa')\n",
    "    else:\n",
    "        evaluator = Evaluator(name=f'ogbl-{args.dataset}')\n",
    "\n",
    "    device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    data, split_edge = loaddataset(args.dataset, args.use_valedges_as_input, args.load)\n",
    "    data = data.to(device)\n",
    "\n",
    "    predfn = predictor_dict[args.predictor]\n",
    "    if args.predictor != \"cn0\":\n",
    "        predfn = partial(predfn, cndeg=args.cndeg)\n",
    "    if args.predictor in [\"cn1\", \"incn1cn1\", \"scn1\", \"catscn1\", \"sincn1cn1\"]:\n",
    "        predfn = partial(predfn, use_xlin=args.use_xlin, tailact=args.tailact, twolayerlin=args.twolayerlin, beta=args.beta)\n",
    "    if args.predictor == \"incn1cn1\":\n",
    "        predfn = partial(predfn, depth=args.depth, splitsize=args.splitsize, scale=args.probscale, offset=args.proboffset, trainresdeg=args.trndeg, testresdeg=args.tstdeg, pt=args.pt, learnablept=args.learnpt, alpha=args.alpha)\n",
    "    \n",
    "    ret = []\n",
    "\n",
    "    for run in range(0, args.runs):\n",
    "        set_seed(run)\n",
    "        if args.dataset in [\"Cora\", \"Citeseer\", \"Pubmed\"]:\n",
    "            data, split_edge = loaddataset(args.dataset, args.use_valedges_as_input, args.load) # get a new split of dataset\n",
    "            data = data.to(device)\n",
    "        bestscore = None\n",
    "        \n",
    "        \n",
    "        model = GCN(data.num_features, args.hiddim, args.hiddim, args.mplayers,\n",
    "                    args.gnndp, args.ln, args.res, data.max_x,\n",
    "                    args.model, args.jk, args.gnnedp,  xdropout=args.xdp, taildropout=args.tdp, noinputlin=args.loadx).to(device)\n",
    "        if args.loadx:\n",
    "            with torch.no_grad():\n",
    "                model.xemb[0].weight.copy_(torch.load(f\"gemb/{args.dataset}_{args.model}_cn1_{args.hiddim}_{run}.pt\", map_location=\"cpu\"))\n",
    "            model.xemb[0].weight.requires_grad_(False)\n",
    "        predictor = predfn(args.hiddim, args.hiddim, 1, args.nnlayers,\n",
    "                           args.predp, args.preedp, args.lnnn).to(device)\n",
    "        if args.loadmod:\n",
    "            keys = model.load_state_dict(torch.load(f\"gmodel/{args.dataset}_{args.model}_cn1_{args.hiddim}_{run}.pt\", map_location=\"cpu\"), strict=False)\n",
    "            print(\"unmatched params\", keys, flush=True)\n",
    "            keys = predictor.load_state_dict(torch.load(f\"gmodel/{args.dataset}_{args.model}_cn1_{args.hiddim}_{run}.pre.pt\", map_location=\"cpu\"), strict=False)\n",
    "            print(\"unmatched params\", keys, flush=True)\n",
    "        \n",
    "\n",
    "        optimizer = torch.optim.Adam([{'params': model.parameters(), \"lr\": args.gnnlr}, \n",
    "           {'params': predictor.parameters(), 'lr': args.prelr}])\n",
    "        \n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            alpha = max(0, min((epoch-5)*0.1, 1)) if args.increasealpha else None\n",
    "            t1 = time.time()\n",
    "            loss = train(model, predictor, data, split_edge, optimizer,\n",
    "                         args.batch_size, args.maskinput, [], alpha)\n",
    "            print(f\"trn time {time.time()-t1:.2f} s\", flush=True)\n",
    "            if True:\n",
    "                t1 = time.time()\n",
    "                results, h = test(model, predictor, data, split_edge, evaluator,\n",
    "                               args.testbs, args.use_valedges_as_input)\n",
    "                print(f\"test time {time.time()-t1:.2f} s\")\n",
    "                if bestscore is None:\n",
    "                    bestscore = {key: list(results[key]) for key in results}\n",
    "                for key, result in results.items():\n",
    "                    writer.add_scalars(f\"{key}_{run}\", {\n",
    "                        \"trn\": result[0],\n",
    "                        \"val\": result[1],\n",
    "                        \"tst\": result[2]\n",
    "                    }, epoch)\n",
    "\n",
    "                if True:\n",
    "                    for key, result in results.items():\n",
    "                        train_hits, valid_hits, test_hits = result\n",
    "                        if valid_hits > bestscore[key][1]:\n",
    "                            bestscore[key] = list(result)\n",
    "                            if args.save_gemb:\n",
    "                                torch.save(h, f\"gemb/{args.dataset}_{args.model}_{args.predictor}_{args.hiddim}.pt\")\n",
    "                            if args.savex:\n",
    "                                torch.save(model.xemb[0].weight.detach(), f\"gemb/{args.dataset}_{args.model}_{args.predictor}_{args.hiddim}_{run}.pt\")\n",
    "                            if args.savemod:\n",
    "                                torch.save(model.state_dict(), f\"gmodel/{args.dataset}_{args.model}_{args.predictor}_{args.hiddim}_{run}.pt\")\n",
    "                                torch.save(predictor.state_dict(), f\"gmodel/{args.dataset}_{args.model}_{args.predictor}_{args.hiddim}_{run}.pre.pt\")\n",
    "                        print(key)\n",
    "                        print(f'Run: {run + 1:02d}, '\n",
    "                              f'Epoch: {epoch:02d}, '\n",
    "                              f'Loss: {loss:.4f}, '\n",
    "                              f'Train: {100 * train_hits:.2f}%, '\n",
    "                              f'Valid: {100 * valid_hits:.2f}%, '\n",
    "                              f'Test: {100 * test_hits:.2f}%')\n",
    "                    print('---', flush=True)\n",
    "        print(f\"best {bestscore}\")\n",
    "        if args.dataset == \"collab\":\n",
    "            ret.append(bestscore[\"Hits@50\"][-2:])\n",
    "        elif args.dataset == \"ppa\":\n",
    "            ret.append(bestscore[\"Hits@100\"][-2:])\n",
    "        elif args.dataset == \"ddi\":\n",
    "            ret.append(bestscore[\"Hits@20\"][-2:])\n",
    "        elif args.dataset == \"citation2\":\n",
    "            ret.append(bestscore[-2:])\n",
    "        elif args.dataset in [\"Pubmed\", \"Cora\", \"Citeseer\"]:\n",
    "            ret.append(bestscore[\"Hits@100\"][-2:])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    ret = np.array(ret)\n",
    "    print(ret)\n",
    "    print(f\"Final result: val {np.average(ret[:, 0]):.4f} {np.std(ret[:, 0]):.4f} tst {np.average(ret[:, 1]):.4f} {np.std(ret[:, 1]):.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_new_env_ncn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
